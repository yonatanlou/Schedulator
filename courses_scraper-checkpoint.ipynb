{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "p3JA4ywsYW_B",
    "outputId": "eecea542-aea7-4bee-d019-5650df05f57e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# set options to be headless, ..\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "#installation guide for selenium\n",
    "#https://medium.com/@shanyitan/how-to-install-selenium-and-run-it-successfully-via-jupyter-lab-c3f50d22a0d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3XpR3qP3YVr5"
   },
   "outputs": [],
   "source": [
    "#gathering information function\n",
    "def gather(browser):\n",
    "    lst=[]\n",
    "    try:\n",
    "        x=1\n",
    "        while True: \n",
    "            x_pather = \"//*[@id='myBody']/table/tbody/tr/td/table/tbody/tr[5]/td/form/table/tbody/tr[3]/td/a[\"+str(x)+\"]\"\n",
    "            main = browser.find_element_by_xpath(x_pather)\n",
    "            x+=1\n",
    "            lst.append(main)\n",
    "    except:\n",
    "        lst.pop()\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4U3rFPMbYVr9"
   },
   "source": [
    "### Scraping the data with selenium\n",
    "*TODO we need to make it run by page\n",
    "\n",
    "*TODO testing with other maslulim (Economics and statistics is good but computers not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "Cq-0cKCaYVr-",
    "outputId": "675b60eb-3259-447d-fc20-4aae33ecf79f"
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()\n",
    "browser.get(\"https://shnaton.huji.ac.il/\")\n",
    "scraped_data = []\n",
    "\n",
    "hug_by_fac = {'01': ['0255', '0176', '0150', '0195', '0145', '0158', '0114', '0118', '0249', '0181', '0192', '0151', '0105', '0142', '0109', '0222', '0200', '0238', '0241', '0240', '0242', '0243', '0231', '0237', '0232', '0239', '0112', '0246', '0124', '0122', '0201', '0101', '0160', '0230', '0155', '0172', '0179', '0156', '0141', '0311', '0143', '0298', '0199', '0113', '0301', '0180', '0108', '0117', '0140', '0300', '0244', '0699', '0157', '0123', '0197', '0198', '0202', '0144', '0115', '0102', '1767', '1506', '1811', '1511', '1057', '1223', '1515', '1411', '1561', '1790', '2990', '1136', '1802', '1084', '1401', '1971', '1248', '1016', '1027', '2995', '2885', '1069'],\n",
    "                 '02': ['0592', '0555', '0579', '0809', '0890', '0577', '0573', '0802', '0821', '0590', '0575', '0501', '0529', '0589', '0581', '0588', '0880', '0560', '0321', '0101', '0553', '0545', '0570', '0566', '0576', '0521', '0591', '0572', '0143', '0595', '0530', '0531', '0596', '0320', '0824', '0543', '0541', '0511', '0318', '0502', '0611', '0569', '0520', '5881', '5882', '5602', '5954', '5701', '5905', '5221', '5601', '5222', '5920', '5210', '5952', '5311', '5422', '5212', '5998'], \n",
    "                 '03': ['0416', '0222', '0405', '0414', '0401', '0411'], \n",
    "                 '04': ['0698', '0603', '0616', '0624', '0602', '0602', '0626', '0618', '0608', '0606', '0610', '0612', '0619', '0630', '0621', '0621', '0640', '0625', '0607', '0627', '0627', '0601', '0681', '0699', '0613'],\n",
    "                 '05': ['0617', '0611'], \n",
    "                 '06': ['0370', '0325', '0343', '0322', '3242', '3227', '1008', '3224', '1006', '3221', '3223', '3226', '1002', '3220', '3245', '3234', '3228', '1004', '3243', '3244'],\n",
    "                 '07': ['0802', '0810', '0821', '0358', '0350', '0312', '0321', '0337', '0364', '0365', '0369', '0335', '0330', '0311', '0816', '0815', '0301', '0320', '0824', '0300', '0318', '0363', '0326', '0323', '3020', '3651', '8030', '8024', '8032', '8022', '8020', '3119', '3017', '3110', '5903'],\n",
    "                 '08': ['0715', '0792', '0728', '0890', '0722', '0793', '0723', '0735', '0737', '0713', '0731', '0721', '0732', '0799', '0795', '0738', '0717', '0718', '0794', '0710', '0791', '0716', '0712', '0733', '0714', '0729', '0730', '7725', '7726'],\n",
    "                 '09': ['0434', '0433', '0431', '0432'], \n",
    "                 '11': ['0597', '5970'], \n",
    "                 '12': ['0525', '0523', '0583', '0587', '0586', '0521', '0532', '0591', '0595', '0322', '0530', '0541', '0520', '0323', '5210', '5311', '5422', '5212'], \n",
    "                 '16': ['0855'],\n",
    "                 '30': ['0728', '0809', '0815', '0502', '8997']}\n",
    "\n",
    "lst_toar = [0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "for faculty in hug_by_fac.keys():\n",
    "    \n",
    "    select = Select(browser.find_element_by_id('faculty'))\n",
    "    select.select_by_value(str(faculty)) #Pass value as string\n",
    "    sleep(1)\n",
    "    \n",
    "    for hug in hug_by_fac[faculty]:\n",
    "        \n",
    "        sleep(0.5)\n",
    "        select = Select(browser.find_element_by_id('faculty'))\n",
    "        select.select_by_value(str(faculty)) #Pass value as string\n",
    "        sleep(1)\n",
    "        select = Select(browser.find_element_by_id('hugim'))\n",
    "        select.select_by_value(str(hug))\n",
    "        sleep(1)\n",
    "        \n",
    "        maslul_options = browser.find_elements(By.XPATH,'//*[@name=\"maslul\"]/option')\n",
    "        #second loop over maslul\n",
    "        maslul_lst = []\n",
    "        for maslul in maslul_options:\n",
    "            if maslul.get_attribute(\"value\") == \"0\" or maslul.text == \"בחרו חוג\":\n",
    "                pass\n",
    "            else:\n",
    "                maslul_lst.append(maslul.get_attribute(\"value\")) \n",
    "        for i in maslul_lst:\n",
    "                \n",
    "                select = Select(browser.find_element_by_id('faculty'))\n",
    "                select.select_by_value(str(faculty)) #Pass value as string\n",
    "                sleep(1)\n",
    "                \n",
    "                select = Select(browser.find_element_by_id('hugim'))\n",
    "                select.select_by_value(str(hug))\n",
    "                sleep(1)\n",
    "\n",
    "                select = Select(browser.find_element_by_id('maslul'))\n",
    "                select.select_by_value(str(i))\n",
    "                sleep(1)\n",
    "\n",
    "                for toar in lst_toar:\n",
    "                    try:\n",
    "                        select = Select(browser.find_element_by_id('toar'))\n",
    "                        select.select_by_value(str(toar))\n",
    "                        sleep(1)\n",
    "\n",
    "                        select = Select(browser.find_element_by_id('shana'))\n",
    "                        select.select_by_value('0')\n",
    "                        sleep(1)\n",
    "\n",
    "                        browser.find_element_by_css_selector(\"input[type='radio'][id='full']\").click()\n",
    "                        sleep(0.2)\n",
    "\n",
    "                        browser.find_element_by_css_selector(\"input[type='button'][value=' חפש ']\").click()\n",
    "                    except:continue\n",
    "                    #collecting information\n",
    "                    try:\n",
    "                        while True:#information gathering function here\n",
    "                            browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "                            sleep(2.5)\n",
    "                            for i in gather(browser):\n",
    "                                scraped_data.append(i.text)\n",
    "                            browser.find_element_by_xpath(\"//a[@title='העמוד הבא']\").click()\n",
    "                    except:\n",
    "                        browser.get(\"https://shnaton.huji.ac.il/\")\n",
    "\n",
    "        \n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6FIZmYBYVsF"
   },
   "source": [
    "### Gathering the data from the scrpaed text\n",
    "gathering the data to dictionaries with regex.\n",
    "we might have some weak points here.\n",
    "#TODO test with other maslulim.\n",
    "\n",
    "#### infrastructure \n",
    "database = {id:[\"semester\": א\\ב\\שנתי, \"day\": א\\ב\\ג\\ד\\ה\\ו, \"hours\": \"hh:mm-hh:mm\",....,..]}\n",
    "\n",
    "from outside to inside:\n",
    "big dictionary keys are for course_id, the value is a list which every item in the list is a different hour of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "id": "2-LcASCfYVsG",
    "outputId": "5123dedf-2677-4993-de53-eaa790f63e35"
   },
   "outputs": [],
   "source": [
    "def data_gatherer(scraped_data):\n",
    "    \"\"\" Return (dictionary that contains all the courses per page, canceled courses)\"\"\"\n",
    "    database = dict()\n",
    "    canceled_courses = []\n",
    "    for course in scraped_data:\n",
    "        course_details = []\n",
    "        info_by_day = dict()\n",
    "\n",
    "        hours = (re.findall(r\"([0-5][0-9]:[0-5][0-9]-[0-5][0-9]:[0-5][0-9])\", course))\n",
    "        course_id = (re.findall(r\"\\|\\s{3}(\\d{5})\", course))\n",
    "        day = (re.findall(r\"\\n(יום [אבגדהו])\", course))\n",
    "        semester = (re.findall(r\"\\n(שנתי|סמסטר [אבגדהו])\", course))\n",
    "  \n",
    "        #checking for canceled courses\n",
    "        if len(course_id) == 0 or len(hours) == 0:\n",
    "            if len(hours) == 0:\n",
    "                canceled_courses.append(course_id)\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            for item in range(len(semester)):\n",
    "                info_by_day={}\n",
    "                info_by_day[\"semester\"] = semester[item] \n",
    "                try:\n",
    "                    info_by_day[\"day\"] = day[item]\n",
    "                except:\n",
    "                    info_by_day[\"day\"] = 'None'\n",
    "                try:\n",
    "                    info_by_day[\"hours\"] = hours[item]\n",
    "                except:\n",
    "                    info_by_day[\"hours\"] = 'None' \n",
    "                course_details.append(info_by_day)\n",
    "            database[course_id[0]] = course_details\n",
    "\n",
    "    canceled_courses = [x for x in canceled_courses if x != []]\n",
    "    return database, canceled_courses\n",
    "data_gatherer(scraped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qiR2-8gHYVsJ",
    "outputId": "ce1a8c1e-c000-4c27-e326-77506f0e6053"
   },
   "outputs": [],
   "source": [
    "#helper for regex\n",
    "counter = 0\n",
    "for i in scraped_data:\n",
    "    print(counter)\n",
    "    print(i,\"\\n\"+\"\\n\"+\"*******end of object********\"+\"\\n\"+\"\\n\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLnFIWT8YVsL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of courses_scraper-checkpoint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
