{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "p3JA4ywsYW_B",
    "outputId": "eecea542-aea7-4bee-d019-5650df05f57e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# set options to be headless, ..\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "#installation guide for selenium\n",
    "#https://medium.com/@shanyitan/how-to-install-selenium-and-run-it-successfully-via-jupyter-lab-c3f50d22a0d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "3XpR3qP3YVr5"
   },
   "outputs": [],
   "source": [
    "#gathering information function\n",
    "def gather(browser):\n",
    "    lst=[]\n",
    "    try:\n",
    "        x=1\n",
    "        while True: \n",
    "            x_pather = \"//*[@id='myBody']/table/tbody/tr/td/table/tbody/tr[5]/td/form/table/tbody/tr[3]/td/a[\"+str(x)+\"]\"\n",
    "            main = browser.find_element_by_xpath(x_pather)\n",
    "            x+=1\n",
    "            lst.append(main)\n",
    "    except:\n",
    "        lst.pop()\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4U3rFPMbYVr9"
   },
   "source": [
    "### Scraping the data with selenium\n",
    "*TODO we need to make it run by page\n",
    "\n",
    "*TODO testing with other maslulim (Economics and statistics is good but computers not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "Cq-0cKCaYVr-",
    "outputId": "675b60eb-3259-447d-fc20-4aae33ecf79f"
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()\n",
    "browser.get(\"https://shnaton.huji.ac.il/\")\n",
    "scraped_data = []\n",
    "hug_by_fac = {'01': ['0255', '0176', '0150', '0195', '0145', '0158', '0114', '0118', '0249', '0181', '0192', '0151', '0105', '0142', '0109', '0222', '0200', '0238', '0241', '0240', '0242', '0243', '0231', '0237', '0232', '0239', '0112', '0246', '0124', '0122', '0201', '0101', '0160', '0230', '0155', '0172', '0179', '0156', '0141', '0311', '0143', '0298', '0199', '0113', '0301', '0180', '0108', '0117', '0140', '0300', '0244', '0699', '0157', '0123', '0197', '0198', '0202', '0144', '0115', '0102', '1767', '1506', '1811', '1511', '1057', '1223', '1515', '1411', '1561', '1790', '2990', '1136', '1802', '1084', '1401', '1971', '1248', '1016', '1027', '2995', '2885', '1069'],\n",
    "                 '02': ['0592', '0555', '0579', '0809', '0890', '0577', '0573', '0802', '0821', '0590', '0575', '0501', '0529', '0589', '0581', '0588', '0880', '0560', '0321', '0101', '0553', '0545', '0570', '0566', '0576', '0521', '0591', '0572', '0143', '0595', '0530', '0531', '0596', '0320', '0824', '0543', '0541', '0511', '0318', '0502', '0611', '0569', '0520', '5881', '5882', '5602', '5954', '5701', '5905', '5221', '5601', '5222', '5920', '5210', '5952', '5311', '5422', '5212', '5998'], \n",
    "                 '03': ['0416', '0222', '0405', '0414', '0401', '0411'], \n",
    "                 '04': ['0698', '0603', '0616', '0624', '0602', '0602', '0626', '0618', '0608', '0606', '0610', '0612', '0619', '0630', '0621', '0621', '0640', '0625', '0607', '0627', '0627', '0601', '0681', '0699', '0613'],\n",
    "                 '05': ['0617', '0611'], \n",
    "                 '06': ['0370', '0325', '0343', '0322', '3242', '3227', '1008', '3224', '1006', '3221', '3223', '3226', '1002', '3220', '3245', '3234', '3228', '1004', '3243', '3244'],\n",
    "                 '07': ['0802', '0810', '0821', '0358', '0350', '0312', '0321', '0337', '0364', '0365', '0369', '0335', '0330', '0311', '0816', '0815', '0301', '0320', '0824', '0300', '0318', '0363', '0326', '0323', '3020', '3651', '8030', '8024', '8032', '8022', '8020', '3119', '3017', '3110', '5903'],\n",
    "                 '08': ['0715', '0792', '0728', '0890', '0722', '0793', '0723', '0735', '0737', '0713', '0731', '0721', '0732', '0799', '0795', '0738', '0717', '0718', '0794', '0710', '0791', '0716', '0712', '0733', '0714', '0729', '0730', '7725', '7726'],\n",
    "                 '09': ['0434', '0433', '0431', '0432'], \n",
    "                 '11': ['0597', '5970'], \n",
    "                 '12': ['0525', '0523', '0583', '0587', '0586', '0521', '0532', '0591', '0595', '0322', '0530', '0541', '0520', '0323', '5210', '5311', '5422', '5212'], \n",
    "                 '16': ['0855'],\n",
    "                 '30': ['0728', '0809', '0815', '0502', '8997']}\n",
    "\n",
    "\n",
    "lst_toar = [0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "hugim_checked = []\n",
    "faculty_checked = []\n",
    "maslul_checked = []\n",
    "\n",
    "for faculty in hug_by_fac.keys():\n",
    "    faculty_checked.append(faculty)\n",
    "    try:\n",
    "        sleep(1)\n",
    "        select = Select(browser.find_element_by_id('faculty'))\n",
    "        select.select_by_value(str(faculty)) #Pass value as string\n",
    "        sleep(1)\n",
    "\n",
    "        for hug in hug_by_fac[faculty]: \n",
    "            hugim_checked.append(hug)\n",
    "            sleep(1.2)\n",
    "            select = Select(browser.find_element_by_id('faculty'))\n",
    "            select.select_by_value(str(faculty)) #Pass value as string\n",
    "            sleep(1.2)\n",
    "            select = Select(browser.find_element_by_id('hugim'))\n",
    "            select.select_by_value(str(hug))\n",
    "            sleep(1.2)\n",
    "\n",
    "            maslul_options = browser.find_elements(By.XPATH,'//*[@name=\"maslul\"]/option')\n",
    "            #second loop over maslul\n",
    "            maslul_lst = []\n",
    "\n",
    "            for mas in maslul_options:\n",
    "                if mas.get_attribute(\"value\") == \"0\" or mas.text == \"בחרו חוג\":\n",
    "                    pass\n",
    "                else:\n",
    "                    maslul_lst.append(mas.get_attribute(\"value\")) \n",
    "\n",
    "            for maslul in maslul_lst:\n",
    "                try:\n",
    "                    if maslul is not None:\n",
    "                        maslul_checked.append(maslul)\n",
    "                    sleep(1.5)\n",
    "                    select = Select(browser.find_element_by_id('faculty'))\n",
    "                    select.select_by_value(str(faculty)) #Pass value as string\n",
    "                    sleep(1.5)\n",
    "\n",
    "                    select = Select(browser.find_element_by_id('hugim'))\n",
    "                    select.select_by_value(str(hug))\n",
    "                    sleep(1.5)\n",
    "\n",
    "                    select = Select(browser.find_element_by_id('maslul'))\n",
    "                    select.select_by_value(str(maslul))\n",
    "                    sleep(1.5)\n",
    "\n",
    "                    for toar in lst_toar:\n",
    "                        try:\n",
    "                            select = Select(browser.find_element_by_id('toar'))\n",
    "                            select.select_by_value(str(toar))\n",
    "                            sleep(1.2)\n",
    "\n",
    "                            select = Select(browser.find_element_by_id('shana'))\n",
    "                            select.select_by_value('0')\n",
    "                            sleep(1.2)\n",
    "\n",
    "                            browser.find_element_by_css_selector(\"input[type='radio'][id='full']\").click()\n",
    "                            sleep(1.2)\n",
    "\n",
    "                            browser.find_element_by_css_selector(\"input[type='button'][value=' חפש ']\").click()\n",
    "                        except:continue\n",
    "                        #collecting information\n",
    "                        try:\n",
    "                            while True:#information gathering function here\n",
    "                                browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "                                sleep(1)\n",
    "                                for i in gather(browser):\n",
    "                                    scraped_data.append(i.text)\n",
    "                                browser.find_element_by_xpath(\"//a[@title='העמוד הבא']\").click()\n",
    "                        except:\n",
    "                            browser.get(\"https://shnaton.huji.ac.il/\")\n",
    "                except:\n",
    "                    browser.get(\"https://shnaton.huji.ac.il/\")\n",
    "\n",
    "    except:\n",
    "        browser.close()\n",
    "        print(\"error in faculty:\"+str(faculty)+\"hug:\"+str(hug)+\"maslul:\"+str(maslul)+\"toar:\"+str(toar))\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the failing scraper checklist\n",
    "print(\"error in faculty: \",str(faculty),\"\\n error in hug: \",str(hug),\"\\n error in maslul: \",str(maslul),\"\\n\")\n",
    "print(\"faculty checked:\",sorted(faculty_checked),\"\\n hugim checked:\",sorted(hugim_checked),\"\\n maslul checked:\",sorted(maslul_checked))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6FIZmYBYVsF"
   },
   "source": [
    "### Gathering the data from the scrpaed text\n",
    "gathering the data to dictionaries with regex.\n",
    "we might have some weak points here.\n",
    "#TODO test with other maslulim.\n",
    "\n",
    "#### infrastructure \n",
    "database = {id:[\"semester\": א\\ב\\שנתי, \"day\": א\\ב\\ג\\ד\\ה\\ו, \"hours\": \"hh:mm-hh:mm\",....,..]}\n",
    "\n",
    "from outside to inside:\n",
    "big dictionary keys are for course_id, the value is a list which every item in the list is a different hour of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manualy continue the scraper\n",
    "scraped_data_copy2 = scraped_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a copy of scraped_data\n",
    "scraped_data_copy =scraped_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data= scraped_data_copy+scraped_data_copy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gatherer(scraped_data):\n",
    "    \"\"\" Return (dictionary that contains all the courses per page, canceled courses)\"\"\"\n",
    "    database = dict()\n",
    "    canceled_courses = []\n",
    "    for course in scraped_data:\n",
    "        course_details = []\n",
    "        info_by_day = dict()\n",
    "        try:\n",
    "            name=re.findall(r\"(\\n[a-z,A-Z]\\D{2,}\\|)\", course)[0].split(\"|\")\n",
    "            name = re.sub(r\"[\\n\\t]*\", \"\", name[0]).replace(\"  \",\" \")\n",
    "\n",
    "        except:\n",
    "            name = 'no name'\n",
    "            \n",
    "        hours = (re.findall(r\"([0-5][0-9]:[0-5][0-9]-[0-5][0-9]:[0-5][0-9])\", course))\n",
    "        course_id = (re.findall(r\"\\|\\s{3}(\\d{5})\", course))\n",
    "        day = (re.findall(r\"\\n(יום [אבגדהו])\", course))\n",
    "        semester = (re.findall(r\"\\n(שנתי|סמסטר [אבגדהו])\", course))\n",
    "          \n",
    "        #checking for canceled courses\n",
    "        if len(course_id) == 0 or len(hours) == 0:\n",
    "            if len(hours) == 0:\n",
    "                canceled_courses.append([course_id,name])\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            for item in range(len(semester)):\n",
    "                info_by_day={}\n",
    "                info_by_day[\"semester\"] = semester[item] \n",
    "                try:\n",
    "                    info_by_day[\"day\"] = day[item]\n",
    "                except:\n",
    "                    info_by_day[\"day\"] = 'None'\n",
    "                try:\n",
    "                    info_by_day[\"hours\"] = hours[item]\n",
    "                except:\n",
    "                    info_by_day[\"hours\"] = 'None' \n",
    "                course_details.append(info_by_day)\n",
    "            database[course_id[0]] = {name:course_details}\n",
    "\n",
    "    canceled_courses = [x for x in canceled_courses if x != []]\n",
    "    return database, canceled_courses\n",
    "database, canceled_courses = data_gatherer(scraped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleaning and sorting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " number of active courses in updated scraper: 4712\n",
      "\n",
      "\n",
      " list of 1309\n"
     ]
    }
   ],
   "source": [
    "\n",
    "database_sorted = {}\n",
    "for key, value in sorted(database.items()): # Note the () after items!\n",
    "    database_sorted[key] = value\n",
    "\n",
    "#removing duplicates and sorting removed\n",
    "dicto = {}\n",
    "for course in canceled_courses:\n",
    "    dicto[str(course)] = None\n",
    "canceled_sorted=sorted((dicto))\n",
    "print(\"\\n\\n number of active courses in updated scraper:\",len(database_sorted.keys()))\n",
    "print(\"\\n\\n list of \"+str(len(canceled_sorted)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### saving a part of the list as text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample= scraped_data[0:30]\n",
    "import json\n",
    "with open('text_sample.txt', 'w') as f:\n",
    "    f.write(json.dumps(text_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To do list:\n",
    "* scraper still getting stuck after a while (due to website maintenance) - maybe using try;except???\n",
    "\n",
    "* saving scraper checkpoints in case of error so we could start from the same checkpoint.\n",
    "\n",
    "note: duplication with courses does not excist due to using a dictionary\n",
    "\n",
    "* adding to each course the group (א,ב,ג,ד....)\n",
    "* adding to each course if מעבדה/תרגול/שיעור/מבחן....\n",
    "\n",
    "* finally creating csv file with all the courses information"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of courses_scraper-checkpoint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
