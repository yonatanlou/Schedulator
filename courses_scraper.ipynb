{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "p3JA4ywsYW_B",
    "outputId": "eecea542-aea7-4bee-d019-5650df05f57e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# set options to be headless, ..\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "#installation guide for selenium\n",
    "#https://medium.com/@shanyitan/how-to-install-selenium-and-run-it-successfully-via-jupyter-lab-c3f50d22a0d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('scraped_data.txt', 'r') as f:\n",
    "    scraped_data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "3XpR3qP3YVr5"
   },
   "outputs": [],
   "source": [
    "#gathering information function\n",
    "def gather(browser):\n",
    "    lst=[]\n",
    "    try:\n",
    "        x=1\n",
    "        while True: \n",
    "            x_pather = \"//*[@id='myBody']/table/tbody/tr/td/table/tbody/tr[5]/td/form/table/tbody/tr[3]/td/a[\"+str(x)+\"]\"\n",
    "            main = browser.find_element_by_xpath(x_pather)\n",
    "            x+=1\n",
    "            lst.append(main)\n",
    "    except:\n",
    "        lst.pop()\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4U3rFPMbYVr9"
   },
   "source": [
    "### Scraping the data with selenium\n",
    "*TODO we need to make it run by page\n",
    "\n",
    "*TODO testing with other maslulim (Economics and statistics is good but computers not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "Cq-0cKCaYVr-",
    "outputId": "675b60eb-3259-447d-fc20-4aae33ecf79f"
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()\n",
    "browser.get(\"https://shnaton.huji.ac.il/\")\n",
    "scraped_data = []\n",
    "hug_by_fac = {'01': ['0255', '0176', '0150', '0195', '0145', '0158', '0114', '0118', '0249', '0181', '0192', '0151', '0105', '0142', '0109', '0222', '0200', '0238', '0241', '0240', '0242', '0243', '0231', '0237', '0232', '0239', '0112', '0246', '0124', '0122', '0201', '0101', '0160', '0230', '0155', '0172', '0179', '0156', '0141', '0311', '0143', '0298', '0199', '0113', '0301', '0180', '0108', '0117', '0140', '0300', '0244', '0699', '0157', '0123', '0197', '0198', '0202', '0144', '0115', '0102', '1767', '1506', '1811', '1511', '1057', '1223', '1515', '1411', '1561', '1790', '2990', '1136', '1802', '1084', '1401', '1971', '1248', '1016', '1027', '2995', '2885', '1069'],\n",
    "                 '02': ['0592', '0555', '0579', '0809', '0890', '0577', '0573', '0802', '0821', '0590', '0575', '0501', '0529', '0589', '0581', '0588', '0880', '0560', '0321', '0101', '0553', '0545', '0570', '0566', '0576', '0521', '0591', '0572', '0143', '0595', '0530', '0531', '0596', '0320', '0824', '0543', '0541', '0511', '0318', '0502', '0611', '0569', '0520', '5881', '5882', '5602', '5954', '5701', '5905', '5221', '5601', '5222', '5920', '5210', '5952', '5311', '5422', '5212', '5998'], \n",
    "                 '03': ['0416', '0222', '0405', '0414', '0401', '0411'], \n",
    "                 '04': ['0698', '0603', '0616', '0624', '0602', '0602', '0626', '0618', '0608', '0606', '0610', '0612', '0619', '0630', '0621', '0621', '0640', '0625', '0607', '0627', '0627', '0601', '0681', '0699', '0613'],\n",
    "                 '05': ['0617', '0611'], \n",
    "                 '06': ['0370', '0325', '0343', '0322', '3242', '3227', '1008', '3224', '1006', '3221', '3223', '3226', '1002', '3220', '3245', '3234', '3228', '1004', '3243', '3244'],\n",
    "                 '07': ['0802', '0810', '0821', '0358', '0350', '0312', '0321', '0337', '0364', '0365', '0369', '0335', '0330', '0311', '0816', '0815', '0301', '0320', '0824', '0300', '0318', '0363', '0326', '0323', '3020', '3651', '8030', '8024', '8032', '8022', '8020', '3119', '3017', '3110', '5903'],\n",
    "                 '08': ['0715', '0792', '0728', '0890', '0722', '0793', '0723', '0735', '0737', '0713', '0731', '0721', '0732', '0799', '0795', '0738', '0717', '0718', '0794', '0710', '0791', '0716', '0712', '0733', '0714', '0729', '0730', '7725', '7726'],\n",
    "                 '09': ['0434', '0433', '0431', '0432'], \n",
    "                 '11': ['0597', '5970'], \n",
    "                 '12': ['0525', '0523', '0583', '0587', '0586', '0521', '0532', '0591', '0595', '0322', '0530', '0541', '0520', '0323', '5210', '5311', '5422', '5212'], \n",
    "                 '16': ['0855'],\n",
    "                 '30': ['0728', '0809', '0815', '0502', '8997']}\n",
    "\n",
    "\n",
    "lst_toar = [0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "hugim_checked = []\n",
    "faculty_checked = []\n",
    "maslul_checked = []\n",
    "\n",
    "for faculty in hug_by_fac.keys():\n",
    "    faculty_checked.append(faculty)\n",
    "    try:\n",
    "        sleep(1)\n",
    "        select = Select(browser.find_element_by_id('faculty'))\n",
    "        select.select_by_value(str(faculty)) #Pass value as string\n",
    "        sleep(1)\n",
    "\n",
    "        for hug in hug_by_fac[faculty]: \n",
    "            hugim_checked.append(hug)\n",
    "            sleep(1.2)\n",
    "            select = Select(browser.find_element_by_id('faculty'))\n",
    "            select.select_by_value(str(faculty)) #Pass value as string\n",
    "            sleep(1.2)\n",
    "            select = Select(browser.find_element_by_id('hugim'))\n",
    "            select.select_by_value(str(hug))\n",
    "            sleep(1.2)\n",
    "\n",
    "            maslul_options = browser.find_elements(By.XPATH,'//*[@name=\"maslul\"]/option')\n",
    "            #second loop over maslul\n",
    "            maslul_lst = []\n",
    "\n",
    "            for mas in maslul_options:\n",
    "                if mas.get_attribute(\"value\") == \"0\" or mas.text == \"בחרו חוג\":\n",
    "                    pass\n",
    "                else:\n",
    "                    maslul_lst.append(mas.get_attribute(\"value\")) \n",
    "\n",
    "            for maslul in maslul_lst:\n",
    "                try:\n",
    "                    if maslul is not None:\n",
    "                        maslul_checked.append(maslul)\n",
    "                    sleep(1.5)\n",
    "                    select = Select(browser.find_element_by_id('faculty'))\n",
    "                    select.select_by_value(str(faculty)) #Pass value as string\n",
    "                    sleep(1.5)\n",
    "\n",
    "                    select = Select(browser.find_element_by_id('hugim'))\n",
    "                    select.select_by_value(str(hug))\n",
    "                    sleep(1.5)\n",
    "\n",
    "                    select = Select(browser.find_element_by_id('maslul'))\n",
    "                    select.select_by_value(str(maslul))\n",
    "                    sleep(1.5)\n",
    "\n",
    "                    for toar in lst_toar:\n",
    "                        try:\n",
    "                            select = Select(browser.find_element_by_id('toar'))\n",
    "                            select.select_by_value(str(toar))\n",
    "                            sleep(1.2)\n",
    "\n",
    "                            select = Select(browser.find_element_by_id('shana'))\n",
    "                            select.select_by_value('0')\n",
    "                            sleep(1.2)\n",
    "\n",
    "                            browser.find_element_by_css_selector(\"input[type='radio'][id='full']\").click()\n",
    "                            sleep(1.2)\n",
    "\n",
    "                            browser.find_element_by_css_selector(\"input[type='button'][value=' חפש ']\").click()\n",
    "                        except:continue\n",
    "                        #collecting information\n",
    "                        try:\n",
    "                            while True:#information gathering function here\n",
    "                                browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "                                sleep(1)\n",
    "                                for i in gather(browser):\n",
    "                                    scraped_data.append(i.text)\n",
    "                                browser.find_element_by_xpath(\"//a[@title='העמוד הבא']\").click()\n",
    "                        except:\n",
    "                            browser.get(\"https://shnaton.huji.ac.il/\")\n",
    "                except:\n",
    "                    browser.get(\"https://shnaton.huji.ac.il/\")\n",
    "\n",
    "    except:\n",
    "        browser.close()\n",
    "        print(\"error in faculty:\"+str(faculty)+\"hug:\"+str(hug)+\"maslul:\"+str(maslul)+\"toar:\"+str(toar))\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the failing scraper checklist\n",
    "print(\"error in faculty: \",str(faculty),\"\\n error in hug: \",str(hug),\"\\n error in maslul: \",str(maslul),\"\\n\")\n",
    "print(\"faculty checked:\",sorted(faculty_checked),\"\\n hugim checked:\",sorted(hugim_checked),\"\\n maslul checked:\",sorted(maslul_checked))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6FIZmYBYVsF"
   },
   "source": [
    "### Gathering the data from the scrpaed text\n",
    "gathering the data to dictionaries with regex.\n",
    "\n",
    "#### infrastructure \n",
    "database = {id:[\"semester\": א\\ב\\שנתי, \"day\": א\\ב\\ג\\ד\\ה\\ו, \"hours\": \"hh:mm-hh:mm\",....,..]}\n",
    "\n",
    "from outside to inside:\n",
    "big dictionary keys are for course_id, the value is a list which every item in the list is a different hour of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gatherer(scraped_data):\n",
    "    \"\"\" Return (dictionary that contains all the courses per page, canceled courses):\n",
    "    database = {course_id:{(course_name, lecturer, sug, group): [{semester:, day:, hours:}]}}\n",
    "    \"\"\"\n",
    "    data_by_name = {}\n",
    "    database = dict()\n",
    "    canceled_courses = []\n",
    "    \n",
    "    for course in scraped_data:\n",
    "        \n",
    "        course_details = []\n",
    "        info_by_day = dict()\n",
    "        try:\n",
    "            name=re.findall(r\"(\\n[a-z,A-Z]\\D{2,}\\|)\", course)[0].split(\"|\")\n",
    "            name = re.sub(r\"[\\n\\t]*\", \"\", name[0]).replace(\"  \",\" \")\n",
    "   \n",
    "        except:\n",
    "            name = 'no name'\n",
    "         \n",
    "        #finding the constants of the courses\n",
    "        course_id = (re.findall(r\"\\|\\s{3}(\\d{5})\", course))#need to improve it\n",
    "        hours = re.findall(r\"([0-5][0-9]:[0-5][0-9]-[0-5][0-9]:[0-5][0-9])\", course)  \n",
    "        group = (re.findall(r\"(?<=\\().?(?=\\))\", course))\n",
    "        \n",
    "        #checking for canceled courses\n",
    "        if len(course_id) == 0 or len(hours) == 0 or name == 'no name' :\n",
    "            if len(hours) == 0:\n",
    "                canceled_courses.append([course_id,name])\n",
    "            pass\n",
    "        \n",
    "        #Find the changing hours within the course\n",
    "        else:\n",
    "            data_by_name[name]=[]\n",
    "            paragraphs = re.findall(r\"\\n((?:\\n.+)+)\", course) #every paragraph supposed to be diffrent sug\n",
    "            hours, day, semester, group,sug,lecturer = [], [], [], [],[],[]\n",
    "            for cell in paragraphs:\n",
    "                hours.append(re.findall(r\"([0-5][0-9]:[0-5][0-9]-[0-5][0-9]:[0-5][0-9])\", cell))\n",
    "                day.append(re.findall(r\"\\n(יום [אבגדהו])\", cell))\n",
    "                semester.append(re.findall(r\"\\n(שנתי|סמסטר [אבגדהו])\", cell))\n",
    "                \n",
    "                sug_lecturer = (re.findall(r\"(תרג|שות|מטלה|הדר|שעור|מעב|שומ)(.+)\", cell))\n",
    "             \n",
    "                try:\n",
    "                    sug = sug_lecturer[0][0]\n",
    "                except:\n",
    "                    sug = \"None\"\n",
    "                    \n",
    "                group = (re.findall(r\"(?<=\\().?(?=\\))\", cell))\n",
    "                try:\n",
    "                    group = group[0]\n",
    "                except:\n",
    "                    group = \"None\"\n",
    "                \n",
    "                try:\n",
    "                    lecturer = sug_lecturer[0][1]\n",
    "                except:\n",
    "                    lecturer = \"None\"\n",
    "                for hour in range(len(hours)):\n",
    "                    time_day = (hours[hour],day[hour])\n",
    "                for sem in range(len(semester)):\n",
    "                    semest = ((semester[sem]))\n",
    "             #   print(sug,group,lecturer,time_day,semester)\n",
    "            #    print(\"   \")\n",
    "                \n",
    "                data_by_name[name].append((sug, group, lecturer,time_day, semest))\n",
    "                \n",
    "            try:\n",
    "                database[course_id[0]] = data_by_name\n",
    "                data_by_name = {}\n",
    "            except:\n",
    "                None\n",
    "    canceled_courses = [x for x in canceled_courses if x != []]\n",
    "    return database, canceled_courses\n",
    "database, canceled_courses = data_gatherer(scraped_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleaning and sorting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " number of active courses in updated scraper: 4380\n",
      "\n",
      "\n",
      " list of 1309\n"
     ]
    }
   ],
   "source": [
    "\n",
    "database_sorted = {}\n",
    "for key, value in sorted(database.items()): # Note the () after items!\n",
    "    database_sorted[key] = value\n",
    "\n",
    "#removing duplicates and sorting removed\n",
    "dicto = {}\n",
    "for course in canceled_courses:\n",
    "    dicto[str(course)] = None\n",
    "canceled_sorted=sorted((dicto))\n",
    "print(\"\\n\\n number of active courses in updated scraper:\",len(database_sorted.keys()))\n",
    "print(\"\\n\\n list of \"+str(len(canceled_sorted)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printing the dictionary (300 first elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08214\n",
      "ACADEMIC WRITING IN ENGLISH FOR AMIRIM STUDENTS כתיבה אקדמית באנגלית לתלמידי אמירים  \n",
      " sug: תרג\n",
      " group: א\n",
      " lecturer:  גב חיה פישר\n",
      " day_hour_semester: [{'hour': '14:15-12:30', 'day': '14:15-12:30', 'semester': '14:15-12:30'}, {'hour': 'יום ה', 'day': 'יום ה', 'semester': 'יום ה'}, {'hour': 'שנתי', 'day': 'שנתי', 'semester': 'שנתי'}]\n",
      "\n",
      "08242\n",
      "Directed Readings קריאה מודרכת  \n",
      " sug: תרג\n",
      " group: א\n",
      " lecturer:  גב מעיין רווה\n",
      " day_hour_semester: [{'hour': '14:15-12:30', 'day': '14:15-12:30', 'semester': '14:15-12:30'}, {'hour': 'יום ד', 'day': 'יום ד', 'semester': 'יום ד'}, {'hour': 'סמסטר א', 'day': 'סמסטר א', 'semester': 'סמסטר א'}]\n",
      "\n",
      "08231\n",
      "THE GOOD AND THE HAPPY: THEMES IN GREEK ETHICS הטובים והמאושרים: סוגיות באתיקה יוונית  \n",
      " sug: שות\n",
      " group: א\n",
      " lecturer:  ד\"ר נלי טאלר\n",
      " day_hour_semester: [{'hour': '20:15-18:30', 'day': '20:15-18:30', 'semester': '20:15-18:30'}, {'hour': 'יום ג', 'day': 'יום ג', 'semester': 'יום ג'}, {'hour': 'סמסטר א', 'day': 'סמסטר א', 'semester': 'סמסטר א'}]\n",
      "\n",
      "08240\n",
      "Text and Context - The Biblical Literature ספרות המקרא: טקסט וקונטקסט  \n",
      " sug: שות\n",
      " group: א\n",
      " lecturer:  ד\"ר רוני גולדשטיין\n",
      " day_hour_semester: [{'hour': '18:15-16:30', 'day': '18:15-16:30', 'semester': '18:15-16:30'}, {'hour': 'יום ג', 'day': 'יום ג', 'semester': 'יום ג'}, {'hour': 'סמסטר א', 'day': 'סמסטר א', 'semester': 'סמסטר א'}]\n",
      "\n",
      "08216\n",
      "France: From Rationalism to Realism צרפת: מרציונליזם לריאליזם  \n",
      " sug: שות\n",
      " group: א\n",
      " lecturer:  פרופ יואב רינון\n",
      " day_hour_semester: [{'hour': '18:15-16:30', 'day': '18:15-16:30', 'semester': '18:15-16:30'}, {'hour': 'יום ג', 'day': 'יום ג', 'semester': 'יום ג'}, {'hour': 'סמסטר ב', 'day': 'סמסטר ב', 'semester': 'סמסטר ב'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def day_hour_semester_organizer(days_hours, semesters):\n",
    "    data = [i for i in days_hours] + [semesters]\n",
    "    lst = []\n",
    "    catagories = len(data)\n",
    "    arguments = len(data[0])\n",
    "    \n",
    "    for group in range(catagories):\n",
    "        for day_hour in range(arguments):\n",
    "            event_dict = dict()\n",
    "            event_dict[\"hour\"] = data[group][day_hour]\n",
    "            event_dict[\"day\"] = data[group][day_hour]\n",
    "            event_dict[\"semester\"] = data[group][day_hour]\n",
    "        lst.append(event_dict)\n",
    "    return lst\n",
    "\n",
    "for course_id in list(database)[:5]:\n",
    "    print(course_id)\n",
    "    for course_name, details in database[course_id].items():\n",
    "        print(course_name)\n",
    "        for shiurim in details:\n",
    "            print(f\"\"\" sug: {shiurim[0]}\\n group: {shiurim[1]}\\n lecturer: {shiurim[2]}\\n day_hour_semester: {day_hour_semester_organizer(shiurim[3], shiurim[4])}\\n\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### saving a part of the list as text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_sample= scraped_data[0:30]\n",
    "# import json\n",
    "# with open('text_sample.txt', 'w') as f:\n",
    "#     f.write(json.dumps(text_sample))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of courses_scraper-checkpoint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
